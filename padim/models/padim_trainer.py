import numpy as np
import torch
import torch.nn.functional as F
from tqdm import tqdm
import os
import json
import time
import sys

# æ·»åŠ ç›¸å¯¹å¯¼å…¥
try:
    from .feature_extractor import FeatureExtractor
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from models.feature_extractor import FeatureExtractor

class PaDiMTrainer:
    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.device = device
        self.feature_extractor = FeatureExtractor().to(device)
        self.feature_extractor.eval()
        
        self.means = None
        self.covs = None
        self.image_size = None
        self.projector = None
        
    def fit(self, dataloader, reduce_dims=100):
    #"""ä½¿ç”¨æ­£å¸¸å›¾åƒè®­ç»ƒPaDiMæ¨¡å‹ (GPUåŠ é€Ÿç‰ˆ)"""
        
        print("å¼€å§‹æå–æ­£å¸¸å›¾åƒçš„ç‰¹å¾...")
        all_features = []

        # ç¬¬ä¸€æ­¥ï¼šæå–æ‰€æœ‰æ­£å¸¸å›¾åƒçš„ç‰¹å¾
        for batch in tqdm(dataloader, desc="æå–ç‰¹å¾"):
            if isinstance(batch, (list, tuple)):
                images = batch[0]
            else:
                images = batch

            images = images.to(self.device)
            with torch.no_grad():  # åŠ ä¸Š no_grad èŠ‚çœæ˜¾å­˜
                features = self._extract_multiscale_features(images)
            # æš‚æ—¶ä¿æŒåœ¨ GPU ä¸Šï¼Œä¸è¦ .cpu().numpy()ï¼Œä¸ºäº†åç»­ GPU åˆ‡ç‰‡åŠ é€Ÿ
            all_features.append(features)

        # åˆå¹¶æ‰€æœ‰ç‰¹å¾ [N, H, W, C] (åœ¨ GPU ä¸Šåˆå¹¶)
        all_features = torch.cat(all_features, dim=0)
        print(f"ç‰¹å¾å½¢çŠ¶: {all_features.shape}")

        # ==========================================
        # æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨éšæœºé€šé“é€‰æ‹©æ›¿ä»£çŸ©é˜µæŠ•å½±
        # ==========================================
        self.selected_indices = None

        if reduce_dims and reduce_dims < all_features.shape[-1]:
            total_dims = all_features.shape[-1]
            print(f"âš¡ [æé€Ÿæ¨ç†ä¼˜åŒ–] ä½¿ç”¨éšæœºé€šé“é€‰æ‹©: {total_dims} -> {reduce_dims}")

            # 1. ç”Ÿæˆéšæœºç´¢å¼• (åªåšä¸€æ¬¡)
            # éšæœºé€‰ reduce_dims ä¸ªä¸é‡å¤çš„é€šé“
            self.selected_indices = torch.randperm(total_dims)[:reduce_dims].to(self.device)

            # 2. ç«‹å³åˆ‡ç‰‡ (é›¶è®¡ç®—é‡)
            # [N, H, W, C] -> åœ¨æœ€åä¸€ä¸ªç»´åº¦ C ä¸Šåˆ‡ç‰‡
            all_features = torch.index_select(all_features, -1, self.selected_indices)

            print(f"âœ… é€šé“é€‰æ‹©å®Œæˆï¼Œå½“å‰å½¢çŠ¶: {all_features.shape}")

        # è½¬å› CPU è¿›è¡Œç»Ÿè®¡è®¡ç®— (å› ä¸ºåæ–¹å·®çŸ©é˜µè®¡ç®—åœ¨ CPU numpy ä¸Šå¯èƒ½æ›´ç¨³å®šï¼Œæˆ–è€…ä½ å¯ä»¥å°è¯•ç”¨ torch.cov åœ¨ GPU ä¸Šç®—)
        print("å°†ç‰¹å¾è½¬ç§»è‡³ CPU è¿›è¡Œç»Ÿè®¡è®¡ç®—...")
        all_features = all_features.cpu().numpy()

        # ç¬¬ä¸‰æ­¥ï¼šä¸ºæ¯ä¸ªä½ç½®è®¡ç®—å¤šå…ƒé«˜æ–¯å‚æ•°
        # ... (è¿™éƒ¨åˆ†ä¿æŒä¸å˜ï¼Œè®¡ç®— means å’Œ covs) ...
        print("=" * 60)
        print("å¼€å§‹è®¡ç®—ç»Ÿè®¡å‚æ•°...")
        N, H, W, C = all_features.shape

        self.means = np.zeros((H, W, C))
        self.covs = np.zeros((H, W, C, C))
        
        print(f"éœ€è¦å¤„ç† {H}Ã—{W} = {H*W} ä¸ªç©ºé—´ä½ç½®")
        print(f"æ¯ä¸ªä½ç½®è®¡ç®— {C}Ã—{C} çš„åæ–¹å·®çŸ©é˜µ")
        print(f"æ€»è®¡ç®—é‡: {H*W} ä¸ªä½ç½® Ã— {C}Ã—{C} çŸ©é˜µ")
        print("=" * 60)
        
        # æ·»åŠ è¯¦ç»†çš„è¿›åº¦æ˜¾ç¤º
        total_positions = H * W
        start_time = time.time()
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¡Œè¿›åº¦
        for i in tqdm(range(H), desc="å¤„ç†è¡Œè¿›åº¦"):
            row_start_time = time.time()
            
            for j in range(W):
                # è·å–æ‰€æœ‰å›¾åƒåœ¨ä½ç½®(i,j)çš„ç‰¹å¾
                patch_features = all_features[:, i, j, :]  # [N, C]
                
                # è®¡ç®—å‡å€¼
                self.means[i, j] = np.mean(patch_features, axis=0)
                
                # è®¡ç®—åæ–¹å·®ï¼ˆæ·»åŠ æ­£åˆ™åŒ–ç¡®ä¿æ•°å€¼ç¨³å®šæ€§ï¼‰
                self.covs[i, j] = np.cov(patch_features.T) + 0.01 * np.eye(C)
            
            # æ¯è¡Œå®Œæˆåæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            current_time = time.time()
            row_elapsed = current_time - row_start_time
            total_elapsed = current_time - start_time
            
            # è®¡ç®—è¿›åº¦å’Œé¢„ä¼°æ—¶é—´
            completed_rows = i + 1
            completed_positions = completed_rows * W
            progress_percent = completed_positions / total_positions * 100
            
            # è®¡ç®—é€Ÿåº¦
            positions_per_second = completed_positions / total_elapsed
            
            # è®¡ç®—å‰©ä½™æ—¶é—´
            remaining_positions = total_positions - completed_positions
            if positions_per_second > 0:
                estimated_remaining = remaining_positions / positions_per_second
            else:
                estimated_remaining = 0
            
            # æ¯2è¡Œæˆ–æ¯åˆ†é’Ÿæ›´æ–°ä¸€æ¬¡è¯¦ç»†ä¿¡æ¯
            if (i % 2 == 0) or (i + 1 == H) or (total_elapsed > 60 and current_time - start_time > 60):
                print(f"ğŸ“Š è¿›åº¦: {completed_rows}/{H} è¡Œ | "
                      f"{completed_positions}/{total_positions} ä½ç½® ({progress_percent:.1f}%)")
                print(f"â±ï¸  æœ¬è¡Œè€—æ—¶: {row_elapsed:.1f}s | "
                      f"æ€»è€—æ—¶: {total_elapsed:.1f}s | "
                      f"é¢„è®¡å‰©ä½™: {estimated_remaining:.1f}s")
                print(f"ğŸš€ å¤„ç†é€Ÿåº¦: {positions_per_second:.2f} ä½ç½®/ç§’")
                print("-" * 50)
        
        # è®­ç»ƒå®Œæˆç»Ÿè®¡
        total_time = time.time() - start_time
        print("=" * 60)
        print("âœ… ç»Ÿè®¡è®¡ç®—å®Œæˆ!")
        print(f"âœ… æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"âœ… å¹³å‡é€Ÿåº¦: {total_positions/total_time:.2f} ä½ç½®/ç§’")
        print(f"âœ… å¤„ç†äº† {total_positions} ä¸ªä½ç½®")
        print("=" * 60)
        
        self.image_size = dataloader.dataset[0][0].shape[-1] if hasattr(dataloader, 'dataset') else 224
        print("PaDiMè®­ç»ƒå®Œæˆ!")
    
    def _extract_multiscale_features(self, images):
        """
        æå–å¹¶èåˆå¤šå°ºåº¦ç‰¹å¾
        """
        features_list = self.feature_extractor(images)
        
        # å°†ç‰¹å¾å›¾è°ƒæ•´åˆ°ç›¸åŒå°ºå¯¸å¹¶æ‹¼æ¥
        target_size = features_list[0].shape[2:]  # ä»¥æœ€å¤§çš„ç‰¹å¾å›¾ä¸ºç›®æ ‡
        
        resized_features = []
        for features in features_list:
            # ä½¿ç”¨åŒçº¿æ€§æ’å€¼è°ƒæ•´ç‰¹å¾å›¾å°ºå¯¸
            if features.shape[2:] != target_size:
                features = F.interpolate(features, size=target_size, 
                                       mode='bilinear', align_corners=False)
            resized_features.append(features)
        
        # åœ¨é€šé“ç»´åº¦æ‹¼æ¥ [B, C1+C2+C3, H, W]
        concatenated = torch.cat(resized_features, dim=1)
        
        # è°ƒæ•´ç»´åº¦ä¸º [B, H, W, C] ä¾¿äºåç»­å¤„ç†
        concatenated = concatenated.permute(0, 2, 3, 1)
        
        return concatenated
    
    def save_model(self, output_dir):
        """
        ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ (åŒ…å«éšæœºç´¢å¼•)
        """
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜å‡å€¼å’Œåæ–¹å·®
        np.save(os.path.join(output_dir, 'means.npy'), self.means)
        np.save(os.path.join(output_dir, 'covs.npy'), self.covs)
        
        # ä¿å­˜é…ç½®ä¿¡æ¯
        config = {
            'image_size': self.image_size,
            'device': self.device,
            'means_shape': self.means.shape if self.means is not None else None,
            'covs_shape': self.covs.shape if self.covs is not None else None
        }
        
        with open(os.path.join(output_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=4)
        
        # ==========================================
        # ä¿®æ”¹ï¼šä¿å­˜éšæœºç´¢å¼•è€Œä¸æ˜¯æŠ•å½±å™¨
        # ==========================================
        if self.selected_indices is not None:
            # ä¿å­˜ä¸º Tensor æ–‡ä»¶
            torch.save(self.selected_indices.cpu(), os.path.join(output_dir, 'selected_indices.pt'))
            print(f"ğŸ’¾ éšæœºç´¢å¼•å·²ä¿å­˜: selected_indices.pt")
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
    


    def load_model(self, model_dir):
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        self.means = np.load(os.path.join(model_dir, 'means.npy'))
        self.covs = np.load(os.path.join(model_dir, 'covs.npy'))
        
        with open(os.path.join(model_dir, 'config.json'), 'r') as f:
            config = json.load(f)
        
        self.image_size = config['image_size']
        
        # åŠ è½½æŠ•å½±å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        projector_path = os.path.join(model_dir, 'projector.pkl')
        if os.path.exists(projector_path):
            import joblib
            self.projector = joblib.load(projector_path)
        
        print(f"âœ… æ¨¡å‹ä» {model_dir} åŠ è½½æˆåŠŸ")
        print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"   - å‡å€¼çŸ©é˜µå½¢çŠ¶: {self.means.shape}")
        print(f"   - åæ–¹å·®çŸ©é˜µå½¢çŠ¶: {self.covs.shape}")
        print(f"   - å›¾åƒå°ºå¯¸: {self.image_size}")

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # ç®€å•çš„åŠŸèƒ½æµ‹è¯•
    print("PaDiM Trainer æ¨¡å—æµ‹è¯•")
    trainer = PaDiMTrainer()
    print(f"è®¾å¤‡: {trainer.device}")
    print("æ¨¡å—åŠ è½½æˆåŠŸ!")